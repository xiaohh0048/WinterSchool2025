{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-kks6oR3fb"
      },
      "source": [
        "# 第一节 PyTorch 基础\n",
        "\n",
        "本节将会非常简短地介绍 PyTorch 的基础。教程主要节选自：\n",
        "\n",
        " - PyTorch 官方的 60 分钟教程 https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        " - UvA 教学材料：https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOwR5BJATia8"
      },
      "source": [
        "PyTorch 是一个开源机器学习框架，可以让用户非常自由地编写自己的神经网络并高效地优化网络。然而，PyTorch 并非唯一的深度学习框架。其他框架还包括 TensorFlow、JAX 和 Caffe。\n",
        "\n",
        "目前，PyTorch 的生态已经非常完善地建立起来，拥有庞大的开发者社区（最初由 Facebook 开发），非常灵活，尤其是在研究方面得到广泛应用。许多当前论文都在 PyTorch 中发布代码。\n",
        "下图是在 research paper 中 PyTorch 和其它框架所占得份额，可以看到 PyTorch 已经占领超过半壁江山。\n",
        "\n",
        "<img src=\"https://github.com/xiaohh0048/WinterSchool2025/blob/main/figures/percentage_repo_2023.png?raw=1\" alt=\"image\" width=700/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WVi4m1vTia9"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QFzXZOUTia9"
      },
      "source": [
        "## Tensor 基础\n",
        "\n",
        "### 初始化和基本运算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_uCidCeTia9"
      },
      "source": [
        "Tensor “张量” 是 PyTorch 中的核心数据结构，相当于 Numpy 数组的升级版本，它不仅具备与 Numpy 数组类似的操作功能，还支持 GPU 加速。  \n",
        "“张量”这一名称是一种概念的推广。\n",
        "\n",
        "例如，向量可以看作是一维张量，而矩阵则是二维张量。在构建神经网络时，我们会使用各种形状和不同维度的张量。\n",
        "\n",
        "大部分操作 Numpy array 的函数，也可以直接用于张量上。由于 Numpy 数组和张量的相似性，可以轻松地在它们之间进行转换（将张量转换为 Numpy 数组，或者反之）.\n",
        "\n",
        "创建张量的方法有很多，其中最简单的一种是调用 `torch.Tensor`，并将所需的形状作为参数传入："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DC83eaahTia-",
        "outputId": "d85302bb-2bec-4f92-bc3e-5b45de8a6be7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.8915e+31,  4.5667e-41, -1.8915e+31,  4.5667e-41],\n",
            "         [-1.8915e+31,  4.5667e-41, -1.8915e+31,  4.5667e-41],\n",
            "         [-1.8915e+31,  4.5667e-41, -1.8915e+31,  4.5667e-41]],\n",
            "\n",
            "        [[-1.8916e+31,  4.5667e-41, -1.8916e+31,  4.5667e-41],\n",
            "         [-1.8916e+31,  4.5667e-41, -1.8915e+31,  4.5667e-41],\n",
            "         [-1.8915e+31,  4.5667e-41, -1.6598e+31,  4.5667e-41]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.Tensor(2, 3, 4)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adiXg8M3Tia-"
      },
      "source": [
        "函数 `torch.Tensor` 会为所需的张量分配内存，但它用的是内存中已有的值，因此非常随机，平时不会这么使用。\n",
        "\n",
        "最常使用的初始化方法，是为张量直接分配特定的值，包括：\n",
        "\n",
        "- **`torch.zeros`**：创建一个所有元素为 0 的张量  \n",
        "- **`torch.ones`**：创建一个所有元素为 1 的张量  \n",
        "- **`torch.rand`**：创建一个张量，其中的值是 0 到 1 之间均匀分布的随机数  \n",
        "- **`torch.randn`**：创建一个张量，其中的值是服从均值为 0、方差为 1 的正态分布的随机数  \n",
        "- **`torch.arange`**：创建一个张量，其中包含从 $N$ 开始、按步长为 1 递增，直到 $M$（不包括 $M$）  \n",
        "- **`torch.Tensor`**（输入列表）：从你提供的列表元素中创建一个张量  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nyZS4MDITia_",
        "outputId": "cc52391e-9892-4eed-e4c9-ad6585902455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor from a (nested) list\n",
        "x = torch.Tensor([[1, 2], [3, 4]])\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5uFNUzJHTia_",
        "outputId": "7a288620-9313-4fe7-8fab-18fe01431174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.8657, 0.0879, 0.1889, 0.6046],\n",
            "         [0.8970, 0.2710, 0.0709, 0.2873],\n",
            "         [0.3397, 0.6929, 0.4163, 0.9748]],\n",
            "\n",
            "        [[0.0684, 0.2467, 0.7958, 0.9573],\n",
            "         [0.7206, 0.3851, 0.6928, 0.8426],\n",
            "         [0.8646, 0.5577, 0.3575, 0.7680]]])\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]\n",
        "x = torch.rand(2, 3, 4)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpJeU1aETia_"
      },
      "source": [
        "常用 `.shape` 和 `.size()` 获取 tensor 的维度信息"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0p0j8qjsTibA",
        "outputId": "e3c03280-f380-4d45-fddd-68aac7d56557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([2, 3, 4])\n",
            "Size: torch.Size([2, 3, 4])\n",
            "Size: 2 3 4\n"
          ]
        }
      ],
      "source": [
        "shape = x.shape\n",
        "print(\"Shape:\", x.shape)\n",
        "\n",
        "size = x.size()\n",
        "print(\"Size:\", size)\n",
        "\n",
        "dim1, dim2, dim3 = x.size()\n",
        "print(\"Size:\", dim1, dim2, dim3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsmNRrDrTibA"
      },
      "source": [
        "用 Numpy array 初始化 tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w5pvlb4ETibA",
        "outputId": "5c251a96-6586-4d7a-8c78-9b86c562ae53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy array: [[1 2]\n",
            " [3 4]]\n",
            "PyTorch tensor: tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np_arr = np.array([[1, 2], [3, 4]])\n",
        "tensor = torch.from_numpy(np_arr)\n",
        "\n",
        "print(\"Numpy array:\", np_arr)\n",
        "print(\"PyTorch tensor:\", tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvLZt0K4TibA"
      },
      "source": [
        "大多数 Numpy 中存在的操作，在 PyTorch 中也有相应的实现。完整的操作列表可以在 [PyTorch 文档](https://pytorch.org/docs/stable/tensors.html#) 中找到，这里我们将重点介绍一些最常用的操作。\n",
        "\n",
        "最简单的操作之一就是两个张量的加法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hm8yVERtTibB",
        "outputId": "810f838d-f6e4-4178-81af-727ea87c2e9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 tensor([[0.5616, 0.8430, 0.8401],\n",
            "        [0.3494, 0.0519, 0.8931]])\n",
            "X2 tensor([[0.1638, 0.7241, 0.2262],\n",
            "        [0.6199, 0.5824, 0.8194]])\n",
            "Y tensor([[0.7254, 1.5671, 1.0663],\n",
            "        [0.9693, 0.6343, 1.7126]])\n"
          ]
        }
      ],
      "source": [
        "x1 = torch.rand(2, 3)\n",
        "x2 = torch.rand(2, 3)\n",
        "y = x1 + x2\n",
        "\n",
        "print(\"X1\", x1)\n",
        "print(\"X2\", x2)\n",
        "print(\"Y\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8AejWeTTibB"
      },
      "source": [
        "调用 `x1 + x2` 会创建一个新的张量，包含两个输入张量的和。然而，我们也可以**in-place**地操作，直接在张量的内存中修改其值。这种操作会直接更改 `x2` 的值，之后无法重新访问 `x2` 的原始值。以下是一个示例：\n",
        "\n",
        "In-place 操作都在最后带有一个 `_` ，比如 `add_`, `mul_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "di2UA_1_TibB",
        "outputId": "5a3fc6f0-7564-41cd-981a-34667a2abafc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 (before) tensor([[0.8068, 0.1186, 0.0807],\n",
            "        [0.1556, 0.3359, 0.8396]])\n",
            "X2 (before) tensor([[0.0825, 0.8085, 0.1526],\n",
            "        [0.9157, 0.8673, 0.2409]])\n",
            "X1 (after) tensor([[0.8068, 0.1186, 0.0807],\n",
            "        [0.1556, 0.3359, 0.8396]])\n",
            "X2 (after) tensor([[0.8893, 0.9270, 0.2333],\n",
            "        [1.0713, 1.2032, 1.0805]])\n"
          ]
        }
      ],
      "source": [
        "x1 = torch.rand(2, 3)\n",
        "x2 = torch.rand(2, 3)\n",
        "print(\"X1 (before)\", x1)\n",
        "print(\"X2 (before)\", x2)\n",
        "\n",
        "x2.add_(x1)\n",
        "print(\"X1 (after)\", x1)\n",
        "print(\"X2 (after)\", x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MbyEiwGTibB"
      },
      "source": [
        "另一个常见操作是改变张量的形状。一个大小为 (2,3) 的张量可以重新组织为任何其他包含相同元素数量的形状（例如大小为 (6)、(3,2) 等）。在 PyTorch 中，这种操作被称为 `view`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wgY6NNqQTibB",
        "outputId": "0580ee57-ae95-40c6-c9d2-a61e5f81dc32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([0, 1, 2, 3, 4, 5])\n",
            "X tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(6)\n",
        "print(\"X\", x)\n",
        "\n",
        "x = x.view(2, 3)\n",
        "print(\"X\", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSSIfRJiTibB"
      },
      "source": [
        "另一个常用的操作是矩阵乘法，它在神经网络的运算中及其重要。通常情况下，我们会有一个输入向量 $\\mathbf{x}$，通过一个可学习的权重矩阵 $\\mathbf{W}$ 进行变换。PyTorch 提供了多种进行矩阵乘法的方法和函数，以下是一些常用的选项：\n",
        "\n",
        "- **`torch.matmul`**：在两个张量之间执行矩阵乘积，其具体行为取决于张量的维度。如果两个输入都是矩阵（即 2 维张量），它会执行标准的矩阵乘积。对于更高维的输入，该函数支持 broadcast 机制（详细信息参见 [文档](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)）。也可以使用 `a @ b` 的形式，类似于 Numpy。  \n",
        "- **`torch.mm`**：在两个矩阵之间执行矩阵乘积，但不支持 broadcast（参见 [文档](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm)）。  \n",
        "- **`torch.bmm`**：支持批量维度的矩阵乘法（非常常用！）。如果第一个张量 $T$ 的形状为 ($b \\times n \\times m$)，第二个张量 $R$ 的形状为 ($b \\times m \\times p$)，则输出 $O$ 的形状为 ($b \\times n \\times p$)。这是通过对 $T$ 和 $R$ 的子矩阵执行 $b$ 次矩阵乘法计算得出的：$O_i = T_i @ R_i$。  \n",
        "- **`torch.einsum`**：使用爱因斯坦求和约定执行矩阵乘法及更复杂的运算\n",
        "\n",
        "通常，我们会使用 `torch.matmul` 或 `torch.bmm`。下面我们可以尝试用 `torch.matmul` 进行一次矩阵乘法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tNqvOGaYTibC",
        "outputId": "408a52a2-916a-4130-ec54-c130ac754b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "W tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n",
            "h tensor([[15, 18, 21],\n",
            "        [42, 54, 66]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(6)\n",
        "x = x.view(2, 3)\n",
        "print(\"X\", x)\n",
        "\n",
        "W = torch.arange(9).view(3, 3) # We can also stack multiple operations in a single line\n",
        "print(\"W\", W)\n",
        "\n",
        "h = torch.matmul(x, W) # Verify the result by calculating it by hand too!\n",
        "print(\"h\", h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JYwfxAxTibC"
      },
      "source": [
        "最后是张量的索引，这也和 Numpy array 类似"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V9X77-6STibC",
        "outputId": "8599e1fc-2d85-437c-e7dc-448eec39f2fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "tensor([1, 5, 9])\n",
            "tensor([0, 1, 2, 3])\n",
            "tensor([3, 7])\n",
            "tensor([[ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(12).view(3, 4)\n",
        "print(\"X\", x)\n",
        "\n",
        "print(x[:, 1])   # Second column\n",
        "\n",
        "print(x[0])      # First row\n",
        "\n",
        "print(x[:2, -1]) # First two rows, last column\n",
        "\n",
        "print(x[1:3, :]) # Middle two rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C8rlMcvTibC"
      },
      "source": [
        "### 自动梯度计算\n",
        "\n",
        "使用 PyTorch 在深度学习项目中的主要原因之一是可以**自动计算函数的梯度/导数**。我们主要会用 PyTorch 来实现神经网络，而神经网络本质上就是一些复杂的函数。如果我们在函数中使用需要学习的权重矩阵，那么这些矩阵就被称为**参数**，或简单地称为**权重**。\n",
        "\n",
        "如果我们的神经网络输出的是单一的标量值，我们会讨论**导数**；但实际上，很多时候网络会有**多个输出变量**，在这种情况下我们称之为**梯度**，它是一个更广泛的概念。\n",
        "\n",
        "给定输入 $\\mathbf{x}$，我们通过对输入进行**操作**来定义函数，这通常包括权重矩阵的矩阵乘法以及与所谓的偏置向量（bias vector）的加法。当我们操作输入时，会自动创建一个**计算图**（computational graph），该图展示了如何从输入到达输出的路径。  \n",
        "PyTorch 是一个**动态计算图（define-by-run）**框架，这意味着我们只需执行所需的操作，PyTorch 会自动跟踪计算图。因此，我们在操作的过程中动态创建了计算图。\n",
        "\n",
        "> **为什么我们需要梯度？**  \n",
        "> 假设我们定义了一个函数（即神经网络），用于从输入向量 $\\mathbf{x}$ 计算出期望的输出 $y$。我们会定义一个**误差度量**，用于衡量网络的错误程度，即它在从 $\\mathbf{x}$ 预测输出 $y$ 时的表现有多差。基于这个误差度量，我们可以利用梯度来**更新**与输出相关的权重 $\\mathbf{W}$，从而使网络在下次输入 $\\mathbf{x}$ 时，其输出更接近我们的期望结果。\n",
        "\n",
        "这里不对梯度的计算进行展示了——我们实际在编写代码时也不需要观察这些梯度的计算和传播，但是我们需要知道，当我们像搭乐高积木一样组建了一个网络时，我们实际写的是正向计算的一切流程，而这时反向梯度应该如何计算已经自动被 PyTorch 实现了。总结一下：我们只需要计算出函数的**输出**，然后就可以请求 PyTorch 自动为我们计算出**梯度**。\n",
        "\n",
        "\n",
        "### GPU支持\n",
        "\n",
        "PyTorch的基本运算都有在GPU上实现的方式，而且是经过优化的。这依赖于 PyTorch 底层的CUDA接口。尤其是，在大的矩阵运算和梯度传播时，放到GPU上计算比CPU快的多，于是我们才能实现在GPU上快速训练一个网络，创造人工智能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiRqWY-JTibC"
      },
      "source": [
        "## `nn.Module`\n",
        "\n",
        "像搭乐高积木一样搭建一个 Module。\n",
        "\n",
        "在 `__init__` 里面，初始化自己要的积木块，包括注册各种可以学习的参数。这些参数一般会定义在内建的 Module 里，比如 `nn.Linear`，也可以利用 `nn.Parameters` 自己定义。\n",
        "整个网络的运行逻辑写在 forward 里。当给定输入 `x`时（也可以多个 Tensor 输入），它将如何参与积木块的计算，得到输出。\n",
        "\n",
        "当我们定义好积木块，并定义好如何进行 forward 计算时，PyTorch Module 已经知道如何进行 backward 计算了（如，对于输出值的任何一个元素，它对所有模型参数空间求梯度是多少）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-iUXV_hRTibD",
        "outputId": "45c89f1e-a79a-4fa0-edc9-0a00240a992d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model:\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Just one layer:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Model params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0260,  0.0175, -0.0839,  ...,  0.0408,  0.0996,  0.0992],\n",
            "        [-0.0973,  0.0591,  0.0876,  ..., -0.0554,  0.0586,  0.0279],\n",
            "        [-0.0360, -0.0724, -0.0851,  ...,  0.0445, -0.0164, -0.0601],\n",
            "        ...,\n",
            "        [-0.0220, -0.0243, -0.0517,  ..., -0.0059, -0.0466,  0.0361],\n",
            "        [ 0.0709, -0.0093,  0.0357,  ...,  0.0012,  0.0207,  0.0680],\n",
            "        [-0.0552, -0.0471,  0.0512,  ...,  0.0797,  0.0119,  0.0337]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-7.7415e-02,  8.6237e-02,  6.9028e-02, -9.9805e-03,  5.3269e-02,\n",
            "        -9.2023e-02,  3.3391e-03, -8.5040e-02,  4.8249e-02,  9.6813e-02,\n",
            "         8.2575e-02, -3.3805e-02, -7.6046e-02, -2.9552e-02,  7.5307e-02,\n",
            "        -3.0074e-02,  1.2469e-02,  9.3531e-02, -2.9555e-02,  6.7729e-02,\n",
            "        -9.4486e-02, -2.1682e-02,  8.8057e-02, -3.7457e-02, -6.7547e-02,\n",
            "         6.6136e-02, -6.6555e-02, -6.7581e-02, -5.0929e-02, -9.7019e-03,\n",
            "         8.3278e-02, -7.7803e-02,  9.1325e-02,  5.3218e-02, -1.4363e-02,\n",
            "        -4.4630e-02,  3.7345e-02,  9.4482e-02,  3.5275e-02, -4.3920e-02,\n",
            "        -4.8474e-02,  2.6397e-03, -2.7373e-04, -1.0528e-02,  1.7465e-02,\n",
            "        -6.0965e-02,  4.7024e-02,  8.1666e-02,  9.5431e-02,  7.7117e-02,\n",
            "        -3.5155e-02, -3.4951e-02, -1.6981e-03,  1.3936e-02,  5.8220e-02,\n",
            "         6.4408e-02, -4.6273e-02,  2.2635e-02,  2.7310e-02, -9.8091e-02,\n",
            "        -4.7359e-02,  5.3561e-02, -1.6532e-02, -5.1976e-02, -9.3277e-02,\n",
            "        -7.5633e-02, -4.8036e-02,  1.8321e-02, -2.3634e-02, -1.9235e-02,\n",
            "        -3.6730e-02,  8.7083e-02, -1.6091e-02, -8.7555e-02,  2.5532e-02,\n",
            "         2.8086e-02, -9.5498e-02, -5.5108e-02, -4.0078e-02, -9.1438e-02,\n",
            "         5.5639e-03, -2.5225e-02,  4.8764e-02,  3.0854e-02,  9.3312e-02,\n",
            "        -5.0289e-02, -7.8874e-02, -2.1568e-02,  2.1269e-02,  4.7627e-02,\n",
            "         7.8012e-02, -6.3570e-02,  6.1227e-02, -8.0283e-02, -2.7386e-02,\n",
            "        -4.0251e-02, -7.3453e-02, -4.6020e-02, -1.5759e-02,  6.2549e-03,\n",
            "        -5.8592e-02, -9.5250e-02,  6.5284e-02,  9.6329e-02,  5.9281e-02,\n",
            "        -2.3806e-02,  9.9004e-02,  3.6976e-02,  7.0565e-02, -4.0323e-02,\n",
            "        -4.4279e-02,  2.3852e-02,  9.5275e-02,  1.6786e-02,  5.3742e-02,\n",
            "         3.4914e-02,  4.1172e-02,  6.9125e-02, -9.2595e-02, -3.6749e-02,\n",
            "        -1.3845e-02, -6.0058e-02,  1.9947e-02,  7.7497e-03,  6.5427e-02,\n",
            "         1.9072e-02, -2.7483e-02,  7.4442e-02,  2.1974e-02, -8.6918e-02,\n",
            "         7.3308e-02, -1.4924e-02, -7.0355e-02, -6.0948e-02,  8.7039e-02,\n",
            "         1.7882e-02, -7.7025e-02, -1.0302e-02,  7.0948e-02, -1.9336e-02,\n",
            "        -9.0300e-03, -5.6391e-02, -6.0156e-03,  4.4244e-02, -7.2151e-04,\n",
            "        -6.9815e-02, -5.6901e-02,  9.5827e-02,  5.2240e-02,  3.2151e-02,\n",
            "        -7.3481e-02, -3.3058e-02, -7.8376e-02,  2.9370e-02,  2.7257e-02,\n",
            "         9.0849e-02,  8.4096e-02, -7.4309e-02, -8.8761e-02, -3.7846e-02,\n",
            "        -3.9900e-02,  6.1036e-02,  1.3253e-02, -2.1738e-02, -6.0574e-02,\n",
            "         6.5298e-02, -6.6741e-02, -9.6398e-02,  9.0794e-02, -4.3824e-02,\n",
            "        -5.2758e-03, -8.6614e-03, -1.9314e-02, -5.3623e-02, -1.6272e-03,\n",
            "        -8.7073e-03, -3.7437e-02,  9.4116e-02,  4.2889e-02,  2.7835e-02,\n",
            "        -4.3537e-02, -2.7538e-02, -4.9916e-02, -2.9620e-02, -2.9265e-02,\n",
            "        -7.7990e-02,  7.2897e-02,  3.0326e-03,  9.0075e-02, -7.3959e-02,\n",
            "        -8.7500e-05, -2.3916e-02,  8.0428e-02, -1.0994e-02,  3.5840e-02,\n",
            "         9.1927e-03,  2.1590e-02,  1.3017e-02,  5.0198e-02, -5.4566e-02],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0067, -0.0228,  0.0409,  ...,  0.0683,  0.0096,  0.0507],\n",
            "        [-0.0347, -0.0556,  0.0187,  ...,  0.0200,  0.0523, -0.0229],\n",
            "        [ 0.0018, -0.0329, -0.0169,  ..., -0.0080,  0.0023, -0.0314],\n",
            "        ...,\n",
            "        [-0.0101,  0.0180, -0.0584,  ...,  0.0330,  0.0225,  0.0535],\n",
            "        [-0.0610, -0.0687,  0.0123,  ...,  0.0483, -0.0177,  0.0531],\n",
            "        [-0.0199,  0.0366,  0.0259,  ..., -0.0706,  0.0196,  0.0104]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0258, -0.0373, -0.0194,  0.0627,  0.0440,  0.0404,  0.0267,  0.0599,\n",
            "        -0.0104, -0.0405], requires_grad=True)\n",
            "\n",
            "\n",
            "Layer params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0067, -0.0228,  0.0409,  ...,  0.0683,  0.0096,  0.0507],\n",
            "        [-0.0347, -0.0556,  0.0187,  ...,  0.0200,  0.0523, -0.0229],\n",
            "        [ 0.0018, -0.0329, -0.0169,  ..., -0.0080,  0.0023, -0.0314],\n",
            "        ...,\n",
            "        [-0.0101,  0.0180, -0.0584,  ...,  0.0330,  0.0225,  0.0535],\n",
            "        [-0.0610, -0.0687,  0.0123,  ...,  0.0483, -0.0177,  0.0531],\n",
            "        [-0.0199,  0.0366,  0.0259,  ..., -0.0706,  0.0196,  0.0104]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0258, -0.0373, -0.0194,  0.0627,  0.0440,  0.0404,  0.0267,  0.0599,\n",
            "        -0.0104, -0.0405], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(100, 200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 10)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "    print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt6bMbfgTibD"
      },
      "source": [
        "以上基本涵盖了后面的讲义中用到的 PyTorch 最基本的知识。关于利用 PyTorch Dataset 搭建数据流等，将放在后文介绍。\n",
        "\n",
        "就实用角度来说，PyTorch 支持的function和Module浩如烟海。对于很多等价的实现，其实有一些内置的函数可以更快地完成，这能让模型的运行更加高效。\n",
        "不过，在ChatGPT时代，平时写PyTorch代码基本上可以由智能助手帮我们完成了..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8pUuYMHNTibD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnBmHPTV3_78"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "weaver-uproot5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}